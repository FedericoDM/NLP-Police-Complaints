{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART Finetuning\n",
    "\n",
    "By: Federico Dominguez Molina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate rouge_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "# Local imports\n",
    "from text_parser import TextParser\n",
    "from summarizer_model import Summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "TEXT_FILES_PATH = (\n",
    "    \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/text_files\"\n",
    ")\n",
    "DATA_PATH = \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add local parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and format training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parsers for summarization\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(TEXT_FILES_PATH, nlp_task=\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.ExcelFile(DATA_PATH + \"/manual_summaries.xlsx\")\n",
    "training_data = training_data.parse(\"manual_summaries\")\n",
    "\n",
    "complete_texts = []\n",
    "processed_manual_summaries = []\n",
    "complaint_files = []\n",
    "\n",
    "# Get local\n",
    "for index, row in training_data.iterrows():\n",
    "    manual_summary = row[\"manual_summary\"]\n",
    "    complaint_file = row[\"complaint\"]\n",
    "\n",
    "    # Get complete, preprocessed text\n",
    "    complete_text = text_parser.file_to_string(f\"{TEXT_FILES_PATH}/{complaint_file}\")\n",
    "\n",
    "    # Preprocess manual summary\n",
    "    processed_manual_summary = text_parser.process_given_text(manual_summary)\n",
    "\n",
    "    complete_texts.append(complete_text)\n",
    "    processed_manual_summaries.append(processed_manual_summary)\n",
    "    complaint_files.append(complaint_file)\n",
    "\n",
    "\n",
    "all_text_files = os.listdir(TEXT_FILES_PATH)\n",
    "\n",
    "# Get indices of files in the training set\n",
    "training_set_indices = []\n",
    "\n",
    "for file in all_text_files:\n",
    "    if file in complaint_files:\n",
    "        training_set_indices.append(all_text_files.index(file))\n",
    "\n",
    "training_set = pd.DataFrame(\n",
    "    {\n",
    "        \"complaint\": complaint_files,\n",
    "        \"complete_text\": complete_texts,\n",
    "        \"manual_summary\": processed_manual_summaries,\n",
    "        \"file_index\": training_set_indices,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_excel(DATA_PATH + \"/training_set.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to finetune BART using the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize data for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    training_set[\"complete_text\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=1024,\n",
    ")\n",
    "tokenized_labels = tokenizer(\n",
    "    training_set[\"manual_summary\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[\"input_ids\"][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels[\"input_ids\"])\n",
    "\n",
    "\n",
    "dataset = CustomDataset(tokenized_inputs, tokenized_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [20:51<00:00, 59.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1251.9204, 'train_samples_per_second': 0.06, 'train_steps_per_second': 0.017, 'train_loss': 3.3641651698521207, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=3.3641651698521207, metrics={'train_runtime': 1251.9204, 'train_samples_per_second': 0.06, 'train_steps_per_second': 0.017, 'train_loss': 3.3641651698521207, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"my_awesome_billsum_model\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=4,\n",
    "#     predict_with_generate=True,\n",
    "#     fp16=True,\n",
    "#     push_to_hub=True,\n",
    "# )\n",
    "\n",
    "# odel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory for model checkpoints\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    # eval_dataset=test_dataset,  # If you have a test dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\tokenizer_config.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\special_tokens_map.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\vocab.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\merges.txt',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\added_tokens.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "model.save_pretrained(DATA_PATH + \"/finetuned_bart_model\")\n",
    "tokenizer.save_pretrained(DATA_PATH + \"/finetuned_bart_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(report, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        report, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    "    )\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuned model\n",
    "\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    DATA_PATH + \"/finetuned_bart_model\"\n",
    ")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(DATA_PATH + \"/finetuned_bart_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint: 1077591.txt\n",
      "Summary: Police responded to a call of a man climbing over a fence, officers approached and instructed him to come outside of the fence. refused to follow the instructions and reached for a firearm. was tased, arrested and transported to the 010th district station. allegations: on october 14, 2015, intake aide denise stewart received a letter from complainant and registered a complaint.\n",
      "\n",
      "\n",
      "Finetuned Summary: Citizens office of police accountability received a complaint on october 14, 2015, on behalf of a man who claims he was beaten and tased by chicago police officers on august 16, 2015. Officer roberto gomez, star 11353 and officer jose carrera, star 12997 were accused of punching, kicking, tasing, pointing a weapon at, and falsely charging him with possession of a weapon. Officer carrera was also accused of tasing the man, in violation of rule 8, of kicking him in the face, and of pointing a gun at him.\n",
      "\n",
      "\n",
      "Complaint: 1088596.txt\n",
      "Summary: The complainant, was driving westbound on roosevelt when he was pulled over by a white police officer driving a marked suv. during the stop, the officer told mr. that his headlight was out and proceeded to issue him a ticket. after being issued the ticket, he drove to his daughter’s house and took pictures of his illuminated head lamps and later went to the district to get information about filing a complaint. copa recommends a finding of unfounded for both allegations -- racial profiling and issuing a false citation.\n",
      "\n",
      "\n",
      "Finetuned Summary:  complainant, was driving westbound on roosevelt road when he was stopped by the police and issued a ticket for a broken headlamp. mr. alleges that the traffic stop was racially motivated. copa recommends a finding of unfounded for both allegations -- racial profiling and issuing a false ticket.\n",
      "\n",
      "\n",
      "Complaint: 2015-1078504.txt\n",
      "Summary: The investigation began under ipra, was transferred to copa on september 15, 2017, and the recommendation(s) set forth herein are the rec ommendation(S) of cop a. civilian office of poli ce accountability. It is alleged that on or about december 17, 2015 the accused used excessive force against during his arre st. not sustained.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 complaints to generate summaries for\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "all_complaints = os.listdir(TEXT_FILES_PATH)\n",
    "random_complaints = random.sample(all_complaints, 5)\n",
    "\n",
    "for complaint in random_complaints:\n",
    "    complaint_text = text_parser.file_to_string(f\"{TEXT_FILES_PATH}/{complaint}\")\n",
    "\n",
    "    # Generate summary - Original model\n",
    "    model = Summarizer(MODEL_NAME, complaint_text)\n",
    "    summary = model.generate_summary(\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Complaint: {complaint}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(\"\\n\")\n",
    "    finetuned_summary = generate_summary(\n",
    "        complaint_text, finetuned_model, finetuned_tokenizer\n",
    "    )\n",
    "\n",
    "    print(f\"Finetuned Summary: {finetuned_summary}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

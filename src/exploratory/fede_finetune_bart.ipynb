{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART Finetuning\n",
    "\n",
    "By: Federico Dominguez Molina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fdmol\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fdmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fdmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets evaluate rouge_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Local imports\n",
    "from text_parser import TextParser\n",
    "from summarizer_model import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "TEXT_FILES_PATH = (\n",
    "    \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/text_files\"\n",
    ")\n",
    "DATA_PATH = \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data\"\n",
    "\n",
    "RANDOM_STATE = 30255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add local parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and format training data\n",
    "\n",
    "I load the Excel file with the gold standard summaries, and I then create a training set that also contains the original text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parsers for summarization\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(TEXT_FILES_PATH, nlp_task=\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.ExcelFile(DATA_PATH + \"/manual_summaries.xlsx\")\n",
    "training_data = training_data.parse(\"manual_summaries\")\n",
    "\n",
    "complete_texts = []\n",
    "processed_manual_summaries = []\n",
    "complaint_files = []\n",
    "\n",
    "# Get local\n",
    "for index, row in training_data.iterrows():\n",
    "    manual_summary = row[\"manual_summary\"]\n",
    "    complaint_file = row[\"complaint\"]\n",
    "\n",
    "    # Get complete, preprocessed text\n",
    "    complete_text = text_parser.file_to_string(f\"{TEXT_FILES_PATH}/{complaint_file}\")\n",
    "\n",
    "    # Preprocess manual summary\n",
    "    processed_manual_summary = text_parser.process_given_text(manual_summary)\n",
    "\n",
    "    complete_texts.append(complete_text)\n",
    "    processed_manual_summaries.append(processed_manual_summary)\n",
    "    complaint_files.append(complaint_file)\n",
    "\n",
    "\n",
    "all_text_files = os.listdir(TEXT_FILES_PATH)\n",
    "\n",
    "# Get indices of files in the training set\n",
    "training_set_indices = []\n",
    "\n",
    "for file in all_text_files:\n",
    "    if file in complaint_files:\n",
    "        training_set_indices.append(all_text_files.index(file))\n",
    "\n",
    "training_set = pd.DataFrame(\n",
    "    {\n",
    "        \"complaint\": complaint_files,\n",
    "        \"complete_text\": complete_texts,\n",
    "        \"manual_summary\": processed_manual_summaries,\n",
    "        \"file_index\": training_set_indices,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_excel(DATA_PATH + \"/training_set.xlsx\")\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    training_set, test_size=0.2, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating tokenizers for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_inputs = tokenizer(\n",
    "    training_set[\"complete_text\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=1024,\n",
    ")\n",
    "tokenized_train_labels = tokenizer(\n",
    "    training_set[\"manual_summary\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_test_inputs = tokenizer(\n",
    "    test_df[\"complete_text\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "tokenized_test_labels = tokenizer(\n",
    "    test_df[\"manual_summary\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[\"input_ids\"][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels[\"input_ids\"])\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(tokenized_train_inputs, tokenized_train_labels)\n",
    "test_dataset = CustomDataset(tokenized_test_inputs, tokenized_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [22:22<00:00, 63.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1342.6816, 'train_samples_per_second': 0.056, 'train_steps_per_second': 0.016, 'train_loss': 5.153852190290179, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=5.153852190290179, metrics={'train_runtime': 1342.6816, 'train_samples_per_second': 0.056, 'train_steps_per_second': 0.016, 'train_loss': 5.153852190290179, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory for model checkpoints\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\tokenizer_config.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\special_tokens_map.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\vocab.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\merges.txt',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\added_tokens.json',\n",
       " 'C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/finetuned_bart_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "model.save_pretrained(DATA_PATH + \"/finetuned_bart_model\")\n",
    "tokenizer.save_pretrained(DATA_PATH + \"/finetuned_bart_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(report, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        report, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    "    )\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuned model\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    DATA_PATH + \"/finetuned_bart_model\"\n",
    ")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(DATA_PATH + \"/finetuned_bart_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model for five random complaints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint: 2009-1027884.txt\n",
      "Summary: The Chicago Police Department launched an investigation into the shooting death of a 16-year-old boy on July 2, 2009. The shooting occurred after a group of 3-4 male/blacks committed a home-invasion style armed robbery at xxxx s. spaulding. Police received an oemc call regarding a person with a gun, and were driving through the same alley, when they found themselves behind the fleeing suspects.\n",
      "\n",
      "\n",
      "Finetuned Summary: The Chicago Police Department launched an investigation into the shooting death of a 16-year-old boy on July 2, 2009. Officer a fired eight shots at subject 1 eight (8) times, hitting him in the back of the neck and right flank. The victim was pronounced dead at the hospital.\n",
      "\n",
      "\n",
      "Complaint: 1089601.txt\n",
      "Summary: Officers were patrolling the vicinity of 7826 s. evans avenue p.m. on may 25, 2018, when several gunshots were heard. The officers observed a black suv (kia sorrento) traveling at a high rate of speed out of the east alley of langley avenue and pursued the vehicle. the pursuit ended on the north side of 67th street between ingleside and ellis avenue. during this time, three to five men exited the car and fled on foot. One of them jumped over a fence and turned in officer vincent’s direction with a gun in his right hand, at which point officervincent discharged his firearm at twice but did not strike him.\n",
      "\n",
      "\n",
      "Finetuned Summary: Officer anthony vincent was in uniform and assigned to gang suppression in the 006th district. He and his partner observed a black suv traveling at a high rate of speed out of the east alley of langley avenue and pursued the vehicle. The pursuit ended on the north side of 67th street between ingleside and ellis avenue, when the kia sorrento crashed. Three to five men exited the car and fled on foot. A witness yelled that had a gun. jumped over a fence and turned in officer v Vincent’s direction with a. gun in his right hand. at which point officer. Vincent discharged his firearm at twice but did not strike him. and the other men were located and arrested with no further incident. Did not sustain any reported injuries.\n",
      "\n",
      "\n",
      "Complaint: 1087894.txt\n",
      "Summary: Chicago police officers a, b, and c (collectively “the officers’) stopped a vehicle when subject 1 failed to use a turn signal. When the officers approached the vehicle, they detected the odor of cannabis and ordered the occupants to exit. Officer b conducted a search of the passenger compartment for cannabis but did not locate any cannabis. After the search was completed, officers removed the handcuffs and permitted the people to return to the car.\n",
      "\n",
      "\n",
      "Finetuned Summary: Chicago police officers a, b, and c were on routine patrol when they observed a silver vehicle, being operated by subject 1, fail to use a turn indicator while parallel parking. The officers detected the odor of cannabis and ordered the occupants to exit. Officer b conducted a search of the vehicle for cannabis but did not locate any cannabis. After the search was completed the officers removed the handcuffs and permitted the occupant to return to the car.\n",
      "\n",
      "\n",
      "Complaint: 2019-0004237.txt\n",
      "Summary: Officer trevor ben allegedly used excessive and unnecessary force when he entered and tried to control a moving vehicle. He also allegedly struck the victim about the head and neck area with his firearm. Officer ben admitted he believed the object was either a weapon or drugs.\n",
      "\n",
      "\n",
      "Finetuned Summary: Officer trevor ben allegedly used excessive and unnecessary force against a man during a traffic stop. The man alleged he was struck about the head and neck area with his firearm. Officer ben admitted he did not observe the object mr. was attempting to conceal, but based on his experience and training, believed it was either a weapon or drugs.\n",
      "\n",
      "\n",
      "Complaint: 1078253.txt\n",
      "Summary: Officer a was at home trying to clear his service weapon when it accidentally discharged. The fired round traveled through officer a ’s front door and entered the apartment across the hall. Officer a immediately contacted the xxxth district and requested a supervisor. evidence technician, beat # xxxx, was requested to recover the round from the neighbor’ s wall.\n",
      "\n",
      "\n",
      "Finetuned Summary:  off duty chicago police officer, officer a, was at home trying to clear his service weapon when it accidentally discharged. The fired round traveled through officer. a ’s front door and entered the apartment across the hall. officer  a was careless, in that, he accidentally di scharged his weapon while trying. toclear it. copa recommends a finding of sustained for the allegation against Officer a.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 complaints to generate summaries for\n",
    "all_complaints = os.listdir(TEXT_FILES_PATH)\n",
    "random_complaints = random.sample(all_complaints, 5)\n",
    "\n",
    "for complaint in random_complaints:\n",
    "    complaint_text = text_parser.file_to_string(f\"{TEXT_FILES_PATH}/{complaint}\")\n",
    "\n",
    "    # Generate summary - Original model\n",
    "    model = Summarizer(MODEL_NAME, complaint_text)\n",
    "    summary = model.generate_summary(\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Complaint: {complaint}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(\"\\n\")\n",
    "    finetuned_summary = generate_summary(\n",
    "        complaint_text, finetuned_model, finetuned_tokenizer\n",
    "    )\n",
    "\n",
    "    print(f\"Finetuned Summary: {finetuned_summary}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create summaries for training set\n",
    "\n",
    "We will use both the finetuned and the original BART model to create summaries for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_summaries = []\n",
    "original_summaries = []\n",
    "\n",
    "for index, row in training_set.iterrows():\n",
    "    complaint = row[\"complete_text\"]\n",
    "    model = Summarizer(MODEL_NAME, complaint)\n",
    "    original_summary = model.generate_summary(\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    finetuned_summary = generate_summary(\n",
    "        complaint, finetuned_model, finetuned_tokenizer\n",
    "    )\n",
    "\n",
    "    finetuned_summaries.append(finetuned_summary)\n",
    "    original_summaries.append(original_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with original and finetuned summaries\n",
    "training_set[\"bart_original_summary\"] = original_summaries\n",
    "training_set[\"bart_finetuned_summary\"] = finetuned_summaries\n",
    "\n",
    "training_set.to_excel(DATA_PATH + \"/training_set_with_summaries.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

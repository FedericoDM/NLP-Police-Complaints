{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis for Police Summary Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Local imports\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_cleaner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextParser\n",
      "File \u001b[1;32mc:\\Users\\fdmol\\Desktop\\MSCAPP\\CAPP30255\\NLP-Police-Complaints\\src\\exploratory\\text_cleaner.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     11\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m )  \u001b[38;5;66;03m# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\u001b[39;00m\n\u001b[0;32m     14\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "\n",
    "from text_cleaner import TextParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/text_files\"\n",
    "CHARS_TO_REMOVE = [\"\\n\", \"§\"]\n",
    "REGEX_PATTERNS = [\n",
    "    r\"civilian office of police accountability\\s+\",\n",
    "    r\"log\\s*\\#\\s*\\d+\",\n",
    "    r\"-\\s*\\d+\\s*\\d+\",\n",
    "    r\"summary report of investigation\\s+\",\n",
    "    r\"i.\\s+executive\\s+summary\",\n",
    "    r\"_+\",\n",
    "    r\"\\s*date of incident:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of incident:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"\\s*location of incident:\\s*\\d+\\w+\\s*\\w+\",\n",
    "    r\"\\s*date of copa notification:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of copa notification:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"applicable rules and laws|\"\n",
    "    r\"conclusion|\"\n",
    "    r\"digital evidence|\"\n",
    "    r\"documentary evidence|\"\n",
    "    r\"legal standard|\",\n",
    "    r\"appendix\\s+.*\",\n",
    "    r\"\\s+deputy chief administrator\\s+\",\n",
    "    r\"\\s+deputy chief investigator\\s+\",\n",
    "    r\"\\s+ibid\\s+\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "I define a class for reading and processing the data, I took some ideas from Matt's analysis to remove headers and other elements that are not relevant to us. This could also help in getting better results for the summarization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextParser:\n",
    "    CHARS_TO_REMOVE = CHARS_TO_REMOVE\n",
    "    REGEX_PATTERNS = REGEX_PATTERNS\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def txt_to_list(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a list\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        lines = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                lines.append(line)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def file_to_string(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a string\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for char in self.CHARS_TO_REMOVE:\n",
    "                    line = line.replace(char, \"\")\n",
    "                text += line\n",
    "\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        # Remove REGEX patterns\n",
    "        for pattern in self.REGEX_PATTERNS:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below wraps HuggingFace's tokenizer and model to generate a summary for each complaint. As I mention below, I tweaked the parameters to get better summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(complaint_text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a summary of a complaint given\n",
    "    the complaint text\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(\n",
    "        complaint_text, return_tensors=\"pt\", max_length=2512, truncation=True\n",
    "    )\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and print the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-1052278.txt\n",
      "=====================================\n",
      "1089507.txt\n",
      "=====================================\n",
      "2019-0004415.txt\n",
      "=====================================\n",
      "2015-1075421.txt\n",
      "=====================================\n",
      "2008-1017636.txt\n",
      "=====================================\n",
      "2017-1087307.txt\n",
      "=====================================\n",
      "1074294.txt\n",
      "=====================================\n",
      "2019-0003101.txt\n",
      "=====================================\n",
      "2021-0002951.txt\n",
      "=====================================\n",
      "2014-1069136.txt\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(PATH)\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 10)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    print(complaint)\n",
    "    print(\"=====================================\")\n",
    "    # print(complaint_text)\n",
    "\n",
    "\n",
    "# complaint_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Task: Summarization\n",
    "\n",
    "I will use this model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration\n",
    "\n",
    "In the cells below, I use the model to generate a summary for ten random complaints. The summaries are of medium quality, depending on each complaint. \n",
    "\n",
    "I did the following to try to improve the quality of the summaries:\n",
    "\n",
    "- Adjuster the `max_length` parameter to limit the length of the summary\n",
    "- Adjusted the `min_length` parameter to ensure the summary is at least a certain length\n",
    "- Adjusted the `num_beams` parameter to increase the number of beams used in beam search\n",
    "- Adjusted `no_repeat_ngram_size` parameter to avoid repeating n-grams in the summary\n",
    "\n",
    "I also experimented with the max_length of the tokens used in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint: 2021-0001489.txt\n",
      "=====================================\n",
      "Summary: officer jason bala is alleged to have been detained without justification. cpd officers entered and searched the residence at apt #2, the civilian office of poli ce accountability summary report of investigation1 date /time/location of incident: january 16, 2020 at approximately 3:53 pm, at or chicago, il, accused committed misconduct through the following acts or omissions: 1. a complaint on behalf of the complainant, via the bureau of internal affairs (“bia”)4.\n",
      "\n",
      "\n",
      "Complaint: 2016-1080339.txt\n",
      "=====================================\n",
      "Summary: (witness): involved individual #1 (wits), #6 (sergeant, employee #, appointed, 1995; white male involved officer #). officer yanked his arm away and pulled away from him and announced that she was a police officer. iv. rules rule 1: insubordination of disrespect toward supervisory member on or off duty. rule 2: any action or conduct which impedes the department’s efforts to achieve its policy and goals.\n",
      "\n",
      "\n",
      "Complaint: 2020-0000630.txt\n",
      "=====================================\n",
      "Summary: officer raymond haran asked sergeant william spyker if she was obstructing the crime scene. he allegedly pushed her, and engaged in an unnecessary verbal altercation with the officer’s orders, iv. rule 7: prohibits a ny failure to promote the department's efforts to achieve its policy and goals. Rule 6: prohibitions disobedience of an order or directive.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(PATH)\n",
    "\n",
    "# Get a random list of 10 complaints\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 3)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    summary = generate_summary(complaint_text, tokenizer, model)\n",
    "    print(f\"Complaint: {complaint}\")\n",
    "    print(\"=====================================\")\n",
    "    print(f\"Summary: {summary}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some next steps could be:\n",
    "\n",
    "- Improve cleaning process to remove irrelevant headers and footers\n",
    "- Try other models\n",
    "- Finetune a model on this dataset (we would need to create a labeled dataset for this, and possibly generate the summaries by hand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

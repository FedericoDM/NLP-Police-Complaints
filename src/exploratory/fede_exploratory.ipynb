{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis for Police Summary Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fdmol\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\fdmol\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "c:\\Users\\fdmol\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "<ipython-input-4-33c173740434>:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was too old on your system - pyarrow 10.0.1 is the current minimum supported version as of this release.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/text_files\"\n",
    "CHARS_TO_REMOVE = [\"\\n\", \"ยง\"]\n",
    "REGEX_PATTERNS = [\n",
    "    r\"civilian office of police accountability\\s+\",\n",
    "    r\"log\\s*\\#\\s*\\d+\",\n",
    "    r\"-\\s*\\d+\\s*\\d+\",\n",
    "    r\"summary report of investigation\\s+\",\n",
    "    r\"i.\\s+executive\\s+summary\",\n",
    "    r\"_+\",\n",
    "    r\"\\s*date of incident:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of incident:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"\\s*location of incident:\\s*\\d+\\w+\\s*\\w+\",\n",
    "    r\"\\s*date of copa notification:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of copa notification:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"applicable rules and laws|\"\n",
    "    r\"conclusion|\"\n",
    "    r\"digital evidence|\"\n",
    "    r\"documentary evidence|\"\n",
    "    r\"legal standard|\",\n",
    "    r\"appendix\\s+.*\",\n",
    "    r\"\\s+deputy chief administrator\\s+\",\n",
    "    r\"\\s+deputy chief investigator\\s+\",\n",
    "    r\"\\s+ibid\\s+\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "I define a class for reading and processing the data, I took some ideas from Matt's analysis to remove headers and other elements that are not relevant to us. This could also help in getting better results for the summarization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextParser:\n",
    "    CHARS_TO_REMOVE = CHARS_TO_REMOVE\n",
    "    REGEX_PATTERNS = REGEX_PATTERNS\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def txt_to_list(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a list\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        lines = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                lines.append(line)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def file_to_string(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a string\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for char in self.CHARS_TO_REMOVE:\n",
    "                    line = line.replace(char, \"\")\n",
    "                text += line\n",
    "\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        # Remove REGEX patterns\n",
    "        for pattern in self.REGEX_PATTERNS:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below wraps HuggingFace's tokenizer and model to generate a summary for each complaint. As I mention below, I tweaked the parameters to get better summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(complaint_text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a summary of a complaint given\n",
    "    the complaint text\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(\n",
    "        complaint_text, return_tensors=\"pt\", max_length=2512, truncation=True\n",
    "    )\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and print the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-1052278.txt\n",
      "=====================================\n",
      "1089507.txt\n",
      "=====================================\n",
      "2019-0004415.txt\n",
      "=====================================\n",
      "2015-1075421.txt\n",
      "=====================================\n",
      "2008-1017636.txt\n",
      "=====================================\n",
      "2017-1087307.txt\n",
      "=====================================\n",
      "1074294.txt\n",
      "=====================================\n",
      "2019-0003101.txt\n",
      "=====================================\n",
      "2021-0002951.txt\n",
      "=====================================\n",
      "2014-1069136.txt\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(PATH)\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 10)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    print(complaint)\n",
    "    print(\"=====================================\")\n",
    "    # print(complaint_text)\n",
    "\n",
    "\n",
    "# complaint_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Task: Summarization\n",
    "\n",
    "I will use this model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration\n",
    "\n",
    "In the cells below, I use the model to generate a summary for ten random complaints. The summaries are of medium quality, depending on each complaint. \n",
    "\n",
    "I did the following to try to improve the quality of the summaries:\n",
    "\n",
    "- Adjuster the `max_length` parameter to limit the length of the summary\n",
    "- Adjusted the `min_length` parameter to ensure the summary is at least a certain length\n",
    "- Adjusted the `num_beams` parameter to increase the number of beams used in beam search\n",
    "- Adjusted `no_repeat_ngram_size` parameter to avoid repeating n-grams in the summary\n",
    "\n",
    "I also experimented with the max_length of the tokens used in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint: 2021-0001489.txt\n",
      "=====================================\n",
      "Summary: officer jason bala is alleged to have been detained without justification. cpd officers entered and searched the residence at apt #2, the civilian office of poli ce accountability summary report of investigation1 date /time/location of incident: january 16, 2020 at approximately 3:53 pm, at or chicago, il, accused committed misconduct through the following acts or omissions: 1. a complaint on behalf of the complainant, via the bureau of internal affairs (โbiaโ)4.\n",
      "\n",
      "\n",
      "Complaint: 2016-1080339.txt\n",
      "=====================================\n",
      "Summary: (witness): involved individual #1 (wits), #6 (sergeant, employee #, appointed, 1995; white male involved officer #). officer yanked his arm away and pulled away from him and announced that she was a police officer. iv. rules rule 1: insubordination of disrespect toward supervisory member on or off duty. rule 2: any action or conduct which impedes the departmentโs efforts to achieve its policy and goals.\n",
      "\n",
      "\n",
      "Complaint: 2020-0000630.txt\n",
      "=====================================\n",
      "Summary: officer raymond haran asked sergeant william spyker if she was obstructing the crime scene. he allegedly pushed her, and engaged in an unnecessary verbal altercation with the officerโs orders, iv. rule 7: prohibits a ny failure to promote the department's efforts to achieve its policy and goals. Rule 6: prohibitions disobedience of an order or directive.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(PATH)\n",
    "\n",
    "# Get a random list of 10 complaints\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 3)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    summary = generate_summary(complaint_text, tokenizer, model)\n",
    "    print(f\"Complaint: {complaint}\")\n",
    "    print(\"=====================================\")\n",
    "    print(f\"Summary: {summary}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some next steps could be:\n",
    "\n",
    "- Improve cleaning process to remove irrelevant headers and footers\n",
    "- Try other models\n",
    "- Finetune a model on this dataset (we would need to create a labeled dataset for this, and possibly generate the summaries by hand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

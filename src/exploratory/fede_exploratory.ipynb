{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis for Police Summary Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/text_files\"\n",
    "CHARS_TO_REMOVE = [\"\\n\"]\n",
    "REGEX_PATTERNS = [\n",
    "    r\"civilian office of police accountability\\s+\",\n",
    "    r\"log\\s*\\#\\s*\\d+\",\n",
    "    r\"-\\s*\\d+\\s*\\d+\",\n",
    "    r\"summary report of investigation\\s+\",\n",
    "    r\"i.\\s+executive\\s+summary\",\n",
    "    r\"_+\",\n",
    "    r\"\\s*date of incident:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of incident:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"\\s*location of incident:\\s*\\d+\\w+\\s*\\w+\",\n",
    "    r\"\\s*date of copa notification:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of copa notification:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"applicable rules and laws|\"\n",
    "    r\"conclusion|\"\n",
    "    r\"digital evidence|\"\n",
    "    r\"documentary evidence|\"\n",
    "    r\"legal standard|\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "I define a class for reading and processing the data, I took some ideas from Matt's analysis to remove headers and other elements that are not relevant to us. This could also help in getting better results for the summarization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextParser:\n",
    "    CHARS_TO_REMOVE = CHARS_TO_REMOVE\n",
    "    REGEX_PATTERNS = REGEX_PATTERNS\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def txt_to_list(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a list\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        lines = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                lines.append(line)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def file_to_string(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a string\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for char in self.CHARS_TO_REMOVE:\n",
    "                    line = line.replace(char, \"\")\n",
    "                text += line\n",
    "\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        # Remove REGEX patterns\n",
    "        for pattern in self.REGEX_PATTERNS:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below wraps HuggingFace's tokenizer and model to generate a summary for each complaint. As I mention below, I tweaked the parameters to get better summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(complaint_text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a summary of a complaint given\n",
    "    the complaint text\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(\n",
    "        complaint_text, return_tensors=\"pt\", max_length=2048, truncation=True\n",
    "    )\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=1500,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and print the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Task: Summarization\n",
    "\n",
    "I will use this model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration\n",
    "\n",
    "In the cells below, I use the model to generate a summary for ten random complaints. The summaries are of medium quality, depending on each complaint. \n",
    "\n",
    "I did the following to try to improve the quality of the summaries:\n",
    "\n",
    "- Adjuster the `max_length` parameter to limit the length of the summary\n",
    "- Adjusted the `min_length` parameter to ensure the summary is at least a certain length\n",
    "- Adjusted the `num_beams` parameter to increase the number of beams used in beam search\n",
    "- Adjusted `no_repeat_ngram_size` parameter to avoid repeating n-grams in the summary\n",
    "\n",
    "I also experimented with the max_length of the tokens used in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint: 2019-0004103.txt\n",
      "=====================================\n",
      "Summary: female, race: hispanic, gender: male, male /inactive officer #2: (complainant ) involved individual: case type: star# n/a, dob: gender : female. ii. it is alleged that on or about october 9, 2019, the accused struck (minor) across her forehead and/or face with his hand. It is also allegedly that he failed to notify the department of his involvement as the perpetrator of child abuse in dc\n",
      "\n",
      "\n",
      "Complaint: 2017-1085798.txt\n",
      "=====================================\n",
      "Summary: officer jaime nunez # 17752 (officer phelan) in marked police vehicle 8333. copa also alleged that officer Nunez failed to write an investigatory stop report on july 3, 2017. v. investigation, 2a. interviews 3copa conducted a thorough and complete investigation of the allegations. ii. involved parties involved officer #: involved civilian #1: ed #, doa:02/19/13, rank: police officer.\n",
      "\n",
      "\n",
      "Complaint: 2016-1081264.txt\n",
      "=====================================\n",
      "Summary: ii. analysis and copa find that the allegations against officer and are not sustained. this investigation was transferred to 2copa on september 15, 2017, and the recommendation(s) set forth herein are the recommendations of the police review authority (ipra)\n",
      "\n",
      "\n",
      "Complaint: 2008-1020388.txt\n",
      "=====================================\n",
      "Summary: Officer b approached subject 1’s car from the front, as he was approaching him. officer a said cpd sgt. d, officere e, and the rest of the team were assigned to conduct surveillance on the target, subject 1, and his residence, on 28 september 2008, at the intersection of 79th and south llyde, where o fficer was shot. officers were conducting surveilla nce prior to ex-ecuting the search warrant \n",
      "\n",
      "\n",
      "Complaint: 2017-1087778.txt\n",
      "=====================================\n",
      "Summary: copa: 10 december 2017 1130 hours 10 December 2017 1344 hours. it is alleged that officer pickett grabbed and tripped over a box causing her to strike her head and fall onto the floor. c. the case report (rd# )5 completed by officer quaid contained the same account of the incident as detailed above.\n",
      "\n",
      "\n",
      "Complaint: 1087439.txt\n",
      "=====================================\n",
      "Summary: and civilian 1 were walking westbound from the parking lot and onto madison ave, chicago, illinois. subject 1 stated the officer asked him his name and he told him, “you’re making this shit up” officer b said on october 19, 2017, in front of xxxx w. madise, at approximately 7:46 pm, witness civil 1 (“civilian 1 ”), gave copa an audio recorded interview in the jury room\n",
      "\n",
      "\n",
      "Complaint: 2019-0005108.txt\n",
      "=====================================\n",
      "Summary: a preponderance of evidence can be described as evidence indicating that it is more likely than not that the alleged conduct occurred. copa ’s special order s03 provides an exception to the “beyond -a-reasonable doubt ” standard required to convict someone of criminal offense.27 if the officers did not unlawfully detain alleg ed that he unlawful ly held him.\n",
      "\n",
      "\n",
      "Complaint: 2009-1030513.txt\n",
      "=====================================\n",
      "Summary: sata n disciples street gang, pronounced dead on the scene. beat 4271b saw subject 1 reach under his seat and reversed to the mouth of the alley. u #09/ officer involved #1: “officer a” (chicago police officer); male/hispanic; 36 years old; on- duty; in civilian dress; year of appointment – 2005 officer’s injuries: none reported. officer c, unit 253, related the same account of incident that was consistent with\n",
      "\n",
      "\n",
      "Complaint: 2010-1032871.txt\n",
      "=====================================\n",
      "Summary: Officer b grabbed subj ect 1’s left wrist, placed a handcuff on it and cuffed him to the gate. during the struggle, both officers and the passenger were on the ground, but subject 1 attempted to flee. he had minor injuries on his left leg, left knee, and left elbow from the battle.\n",
      "\n",
      "\n",
      "Complaint: 2022-0005382.txt\n",
      "=====================================\n",
      "Summary: officer #1: female, hispanic involved officer #2: involved individual #1 : male, hep. applicable rules rule 2: any action or conduct which impedes department’s efforts to achieve its goals or brings discredit upon the department. iv. credibility assessment this investigation did not reveal any evidence that po zapata had any prior knowledge of previous criminal charges or convictions, copa finds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(PATH)\n",
    "\n",
    "# Get a random list of 10 complaints\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 10)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    summary = generate_summary(complaint_text, tokenizer, model)\n",
    "    print(f\"Complaint: {complaint}\")\n",
    "    print(\"=====================================\")\n",
    "    print(f\"Summary: {summary}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some next steps could be:\n",
    "\n",
    "- Improve cleaning process to remove irrelevant headers and footers\n",
    "- Try other models\n",
    "- Finetune a model on this dataset (we would need to create a labeled dataset for this, and possibly generate the summaries by hand)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis for Police Summary Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Local imports\n",
    "from text_cleaner import TextParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/text_files\"\n",
    "CHARS_TO_REMOVE = [\"\\n\", \"ยง\"]\n",
    "REGEX_PATTERNS = [\n",
    "    r\"civilian office of police accountability\\s+\",\n",
    "    r\"log\\s*\\#\\s*\\d+\",\n",
    "    r\"-\\s*\\d+\\s*\\d+\",\n",
    "    r\"summary report of investigation\\s+\",\n",
    "    r\"i.\\s+executive\\s+summary\",\n",
    "    r\"_+\",\n",
    "    r\"\\s*date of incident:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of incident:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"\\s*location of incident:\\s*\\d+\\w+\\s*\\w+\",\n",
    "    r\"\\s*date of copa notification:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of copa notification:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"applicable rules and laws|\"\n",
    "    r\"conclusion|\"\n",
    "    r\"digital evidence|\"\n",
    "    r\"documentary evidence|\"\n",
    "    r\"legal standard|\",\n",
    "    r\"appendix\\s+.*\",\n",
    "    r\"\\s+deputy chief administrator\\s+\",\n",
    "    r\"\\s+deputy chief investigator\\s+\",\n",
    "    r\"\\s+ibid\\s+\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "I define a class for reading and processing the data, I took some ideas from Matt's analysis to remove headers and other elements that are not relevant to us. This could also help in getting better results for the summarization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below wraps HuggingFace's tokenizer and model to generate a summary for each complaint. As I mention below, I tweaked the parameters to get better summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(complaint_text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a summary of a complaint given\n",
    "    the complaint text\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(\n",
    "        complaint_text, return_tensors=\"pt\", max_length=2512, truncation=True\n",
    "    )\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=1200,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and print the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-0000577.txt\n",
      "=====================================\n",
      "1089956.txt\n",
      "=====================================\n",
      "2018-1089681.txt\n",
      "=====================================\n",
      "2017-1084713.txt\n",
      "=====================================\n",
      "2018-1089882.txt\n",
      "=====================================\n",
      "1088316.txt\n",
      "=====================================\n",
      "1090499.txt\n",
      "=====================================\n",
      "2015-1083058.txt\n",
      "=====================================\n",
      "1092634.txt\n",
      "=====================================\n",
      "2017-1087409.txt\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(PATH, nlp_task=\"summarization\")\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 10)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    print(complaint)\n",
    "    print(\"=====================================\")\n",
    "    # print(complaint_text)\n",
    "\n",
    "\n",
    "# complaint_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Task: Summarization\n",
    "\n",
    "I will use this model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration\n",
    "\n",
    "In the cells below, I use the model to generate a summary for ten random complaints. The summaries are of medium quality, depending on each complaint. \n",
    "\n",
    "I did the following to try to improve the quality of the summaries:\n",
    "\n",
    "- Adjuster the `max_length` parameter to limit the length of the summary\n",
    "- Adjusted the `min_length` parameter to ensure the summary is at least a certain length\n",
    "- Adjusted the `num_beams` parameter to increase the number of beams used in beam search\n",
    "- Adjusted `no_repeat_ngram_size` parameter to avoid repeating n-grams in the summary\n",
    "\n",
    "I also experimented with the max_length of the tokens used in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint: 2019-0003037.txt\n",
      "=====================================\n",
      "Summary: alleged that on july 22, 2019, at approximately 10:37 pm, at or near 803 w. 80th st., you: 1. performed a traffic stop on without justification, in violation of rule 2 and rule 8. iv. exonerated.\n",
      "\n",
      "\n",
      "Complaint: 2008-1018328.txt\n",
      "=====================================\n",
      "Summary: a gunshot wound to the right shoulder and multiple rib fractures. witness 7 was driving southbound on the i expressway when subject 2 accelerated and drove toward officer f. Witness 7, stated that he did not witness the police involved shooting. witnesses 6 provided an account that was consistent with the description of events that were docum ented in the report of assistant deputy superintendent.\n",
      "\n",
      "\n",
      "Complaint: 2016-1079728.txt\n",
      "=====================================\n",
      "Summary: officer roldan had a gun in his right hand and his left hand was on mr. neck. giovanni was walking to his parked car near his apartment building and yelling from inside ipra on march 20, 2016, swung his service weapon and began searching he walked away from the officer's t-shirt, and then returned to the building. officer was not cooperating with officer, but did not see that officer.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_parser = TextParser(PATH, nlp_task=\"summarization\")\n",
    "\n",
    "# Get a random list of 10 complaints\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 3)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    summary = generate_summary(complaint_text, tokenizer, model)\n",
    "    print(f\"Complaint: {complaint}\")\n",
    "    print(\"=====================================\")\n",
    "    print(f\"Summary: {summary}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some next steps could be:\n",
    "\n",
    "- Improve cleaning process to remove irrelevant headers and footers\n",
    "- Try other models\n",
    "- Finetune a model on this dataset (we would need to create a labeled dataset for this, and possibly generate the summaries by hand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

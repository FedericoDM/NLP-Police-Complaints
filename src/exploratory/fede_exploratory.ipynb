{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis for Police Summary Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "     ---------------------------------------- 0.0/129.4 kB ? eta -:--:--\n",
      "     ------------------------------------ - 122.9/129.4 kB 3.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 129.4/129.4 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.1-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fdmol\\desktop\\mscapp\\capp30255\\.venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/8.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.5/8.4 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.5/8.4 MB 18.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.2/8.4 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.3/8.4 MB 18.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.6/8.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.1/8.4 MB 18.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.4/8.4 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.4/8.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.4/8.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.4/8.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 14.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 330.1/330.1 kB 10.3 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.7/144.7 kB 8.4 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.5/269.5 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.6/269.6 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.9/2.2 MB 56.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.2 MB 20.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 22.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 0.0/170.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 170.9/170.9 kB 10.0 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, pyyaml, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.20.3 pyyaml-6.0.1 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.1 transformers-4.37.2\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/fdmol/Desktop/MSCAPP/CAPP30255/NLP-Police-Complaints/data/text_files\"\n",
    "CHARS_TO_REMOVE = [\"\\n\"]\n",
    "REGEX_PATTERNS = [\n",
    "    r\"civilian office of police accountability\\s+\",\n",
    "    r\"log\\s*\\#\\s*\\d+\",\n",
    "    r\"-\\s*\\d+\\s*\\d+\",\n",
    "    r\"summary report of investigation\\s+\",\n",
    "    r\"i.\\s+executive\\s+summary\",\n",
    "    r\"_+\",\n",
    "    r\"\\s*date of incident:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of incident:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"\\s*location of incident:\\s*\\d+\\w+\\s*\\w+\",\n",
    "    r\"\\s*date of copa notification:\\s*\\w+\\s+\\d+,\\s+\\d+\",\n",
    "    r\"\\s*time of copa notification:\\s*\\d+:\\d+\\w+\",\n",
    "    r\"applicable rules and laws|\"\n",
    "    r\"conclusion|\"\n",
    "    r\"digital evidence|\"\n",
    "    r\"documentary evidence|\"\n",
    "    r\"legal standard|\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "I define a class for reading a processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextParser:\n",
    "    CHARS_TO_REMOVE = CHARS_TO_REMOVE\n",
    "    REGEX_PATTERNS = REGEX_PATTERNS\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def txt_to_list(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a list\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        lines = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                lines.append(line)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def file_to_string(self, filename):\n",
    "        \"\"\"\n",
    "        Add each line of a text file to a string\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        file_path = os.path.join(self.path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for char in self.CHARS_TO_REMOVE:\n",
    "                    line = line.replace(char, \"\")\n",
    "                text += line\n",
    "\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        # Remove REGEX patterns\n",
    "        for pattern in self.REGEX_PATTERNS:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below wraps HuggingFace's tokenizer and model to generate a summary for each complaint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(complaint_text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a summary of a complaint given\n",
    "    the complaint text\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(\n",
    "        complaint_text, return_tensors=\"pt\", max_length=2048, truncation=True\n",
    "    )\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=1500,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and print the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Task: Summarization\n",
    "\n",
    "I will use this model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_parser = TextParser(PATH)\n",
    "\n",
    "# Get a random list of 10 complaints\n",
    "complaints = os.listdir(PATH)\n",
    "complaints = [complaint for complaint in complaints if complaint.endswith(\".txt\")]\n",
    "complaints = random.sample(complaints, 10)\n",
    "\n",
    "\n",
    "for complaint in complaints:\n",
    "    complaint_text = text_parser.file_to_string(complaint)\n",
    "    summary = generate_summary(complaint_text, tokenizer, model)\n",
    "    print(f\"Complaint: {complaint}\")\n",
    "    print(\"=====================================\")\n",
    "    print(f\"Summary: {summary}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
